{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief\n",
    "\n",
    "In gradient based bigram model, instead of explicitly counting co-occurrence of bigrams, we will randomly initialze those counts and use gradient descent to fill the counts automatically. The gradient will descent according to negative log likelihood loss function.\n",
    "\n",
    "### Action plan:\n",
    "1. Read the data and construct input, output for the neural net which will look something like following:\n",
    "    -   names = ['juho', ...]\n",
    "    -   `Xs = ['<s>', 'j', 'u', 'h', 'o', ...]` \n",
    "    -   `Ys = ['j', 'u', 'h', 'o', '<e>', ...]`\n",
    "    -   then use `c2i` to map each character into an int\n",
    "    -   The reason is that we want to feed in a neural net a previous character and expect the model to output the next character.\n",
    "2. Create a weight matrix that will resemble probability matrix in the orginal bigram model.\n",
    "    -   It will be a matrix of shape `V x V` where `V` is our character space.\n",
    "    -   `V[i,j]` can be interpreted as the prob. of `i2c[i]` preceding `i2c[j]`\n",
    "    -    To do so, we need to define:\n",
    "           1. appropriate shape of the matrix\n",
    "           2. appropriate functions to be applied so that we get probability distribution in each row of the matrix,\n",
    "           3. and an appropriate loss function to compare the resulting prob. distribution to the real distribution.\n",
    "    -   The model output would be something like `[0.05 0.65 0.30]` where the reference is `[0, 1, 0]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import names, chars, c2i, i2c\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = [], []\n",
    "\n",
    "for name in names:\n",
    "    bigrams = [(c2i[a],c2i[b]) for a,b in zip(['<s>'] + list(name), list(name) + ['<e>'])]\n",
    "\n",
    "    Xs.extend([a for a,_ in bigrams])\n",
    "    Ys.extend([b for _,b in bigrams])\n",
    "\n",
    "Xs = torch.tensor(Xs)\n",
    "Ys = torch.tensor(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  2,  7,  ...,  5, 18, 43]),\n",
       " tensor([ 2,  7, 26,  ..., 18, 43, 46]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "Xenc = F.one_hot(Xs, num_classes=len(c2i)).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((len(c2i), len(c2i)))\n",
    "logits = Xenc @ W\n",
    "\n",
    "# Two below lines are just Softmax to\n",
    "# change logits into prob. dist.\n",
    "probs = logits.exp()\n",
    "probs /= probs.sum(dim=1, keepdims=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get probability distribution of each to-be next character. We then compare this probability to the real one-hotted following character using loss function. Note here that the gradient can be propagated through all the opeartion we have applied here since all of them are differentiable.\n",
    "\n",
    "The loss function that we are using here is Cross-entropy loss which is:\n",
    "\n",
    "$$\\textbf L(\\hat y, y) = -\\sum_{c \\in \\textbf{C}} y_c \\log{\\hat y_c}$$\n",
    "\n",
    "Now for this task the loss is reduced to be:\n",
    "\n",
    "$$\\textbf L(\\hat p, p) = -1 * log(\\hat p_c)$$\n",
    "\n",
    "where $\\hat p_c$ is the predicted probability of character $c$, and the correct next character is `i2c[c]` (so, $p_c = 1$)\n",
    "\n",
    "Then, for a minibatch, we need to reduce loss by some aggregation function such as sum or average to make the loss for a minibatch scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((len(c2i), len(c2i)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5159099102020264\n",
      "2.515619993209839\n",
      "2.5153369903564453\n",
      "2.5150606632232666\n",
      "2.514791488647461\n",
      "2.514528274536133\n",
      "2.5142714977264404\n",
      "2.5140209197998047\n",
      "2.513775587081909\n",
      "2.513536214828491\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = Xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = -1* probs[ torch.arange(len(Xenc)), Ys ].log()\n",
    "    reduced_loss = loss.mean()\n",
    "\n",
    "    if i%10==0:\n",
    "        print(reduced_loss.item()) # we need to reduce loss to make it scalar\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    reduced_loss.backward()\n",
    "\n",
    "    W.data -= 20* W.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kidren\n",
      "kaimikebosvars\n",
      "an\n",
      "jatr\n",
      "ary\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    tokens = [c2i['<s>']]\n",
    "\n",
    "    while True:\n",
    "        recent_token = torch.tensor([tokens[-1]])\n",
    "        input_vector = F.one_hot(recent_token, num_classes=len(c2i)).float()\n",
    "        logits_output = input_vector @ W\n",
    "        counts_output = logits_output.exp()\n",
    "        probs_output = counts_output / counts_output.sum(1, keepdims=True)\n",
    "\n",
    "        next_idx = torch.multinomial(probs_output, num_samples = 1).item()\n",
    "        tokens.append(next_idx)\n",
    "        \n",
    "        if i2c[next_idx] == '<e>':\n",
    "            break\n",
    "\n",
    "    print(''.join([i2c[i] for i in tokens][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93d343a67186057eba7cc3f4fb38dbcf94a41bc6d951d30420561a381ab22bcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
